# 自注意力机制（Self-attention）核心原理与应用

## 1. 自注意力机制基础

### 1.1 核心概念
自注意力机制通过动态计算输入序列中每个位置与其他位置的关联权重，捕捉长距离依赖关系。其核心组件包括：
- **查询（Query）**：当前关注的位置向量。
- **键（Key）**：用于与查询匹配的向量，决定注意力权重。
- **值（Value）**：携带具体信息的向量，用于加权求和生成输出。

### 1.2 计算流程
1. **注意力分数计算**：  
   \[
   \alpha_{i,j} = \frac{\text{Query}_i \cdot \text{Key}_j}{\sqrt{d_k}} \quad (\text{缩放点积注意力})
   \]
2. **Softmax归一化**：  
   \[
   \alpha'_{i,j} = \text{Softmax}(\alpha_{i,j}) = \frac{\exp(\alpha_{i,j})}{\sum_k \exp(\alpha_{i,k})}
   \]
3. **加权求和生成输出**：  
   \[
   \text{Output}_i = \sum_j \alpha'_{i,j} \cdot \text{Value}_j
   \]

---

## 2. 多头自注意力（Multi-head Self-attention）

### 2.1 动机
- 单一注意力头可能无法捕捉多种类型的相关性（如语法、语义）。
- 多头机制通过并行学习多组独立的注意力模式，增强模型表达能力。

### 2.2 实现步骤
1. **线性投影**：将输入分别映射到多组Query、Key、Value空间。
   \[
   Q^h = W^h_Q X, \quad K^h = W^h_K X, \quad V^h = W^h_V X \quad (h=1,...,H)
   \]
2. **独立计算注意力**：每组头生成局部输出。
3. **拼接与线性变换**：合并多头结果并降维。
   \[
   \text{Output} = W_O [\text{head}_1; \text{head}_2; ...; \text{head}_H]
   \]

---

## 3. 位置编码（Positional Encoding）

### 3.1 必要性
自注意力机制本身无位置感知能力，需显式引入位置信息。

### 3.2 常见方法
| 方法            | 特点                                                                 |
|-----------------|--------------------------------------------------------------------|
| **Sinusoidal**  | 固定函数生成（如正弦/余弦波），无需学习，支持任意长度序列。               |
| **Embedding**   | 可学习的嵌入向量，需预定义最大序列长度，适合训练数据充足的场景。          |
| **Relative**    | 编码相对位置（如Shaw et al.），适合长序列任务（如机器翻译）。            |

### 3.3 数学表示（Sinusoidal示例）
\[
PE_{(pos,2i)} = \sin(pos / 10000^{2i/d}), \quad PE_{(pos,2i+1)} = \cos(pos / 10000^{2i/d})
\]

---

## 4. 自注意力的应用

### 4.1 自然语言处理（NLP）
- **Transformer**：完全基于自注意力的编码器-解码器架构，取代传统RNN。
- **BERT**：通过掩码语言模型（MLM）预训练，利用双向自注意力捕捉上下文。

### 4.2 语音处理
- **语音识别**：将声谱图切分为帧序列，使用截断自注意力（Truncated Self-attention）处理长序列。
- **语音合成**：自回归Transformer生成连贯语音波形。

### 4.3 图像处理
- **Vision Transformer (ViT)**：将图像分割为16x16图块，视为序列输入。
- **Detection Transformer (DETR)**：端到端目标检测，无需锚框和后处理。

---

## 5. 自注意力与其他模型的对比

### 5.1 自注意力 vs. CNN
| **特性**                | **自注意力**                              | **CNN**                          |
|-------------------------|------------------------------------------|----------------------------------|
| **感受野**              | 全局（可学习）                            | 局部（固定）                      |
| **参数效率**            | 高（参数共享程度低）                      | 更高（局部连接+参数共享）          |
| **数据需求**            | 需大量数据（ViT需数亿样本）                | 中小规模数据即有效（ResNet）       |

### 5.2 自注意力 vs. RNN
| **特性**                | **自注意力**                              | **RNN**                          |
|-------------------------|------------------------------------------|----------------------------------|
| **并行性**              | 完全并行                                  | 序列依赖，难以并行                |
| **长距离依赖**          | 直接捕捉（无梯度消失）                    | 需复杂结构（如LSTM、GRU）         |
| **内存消耗**            | 高（存储注意力矩阵）                      | 低（仅维护隐状态）                |

---

## 6. 关键结论
1. **动态权重分配**：自注意力通过动态计算位置间权重，灵活捕捉复杂依赖。
2. **多头增强表达**：多头机制提升模型对不同语义关系的建模能力。
3. **位置编码必要性**：需显式编码位置信息以补充序列顺序。
4. **跨领域适用性**：自注意力在NLP、语音、图像等领域均有成功应用。
5. **计算代价权衡**：全局注意力带来高计算复杂度，需结合稀疏或局部优化（如Longformer）。

> **参考资料**  
> - 李宏毅《自注意力机制》课程视频：[链接](https://youtu.be/X7PH3NuYWQQ)  
> - Transformer原论文：[Attention Is All You Need](https://arxiv.org/abs/1706.03762)  
> - Vision Transformer论文：[An Image is Worth 16x16 Words](https://arxiv.org/abs/2010.11929)
