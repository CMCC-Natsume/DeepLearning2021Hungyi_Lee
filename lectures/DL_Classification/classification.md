# 分类问题与Softmax回归

## 1. 回归与分类的区别
- **回归**：预测连续值，通过比较真实值和预测值的均方误差（MSE）优化模型。
- **分类**：输出类别概率分布，通常分为两步：
  1. 模型输出一个**logits向量**（未归一化的预测值）。
  2. 通过**Softmax函数**将logits转换为概率分布。

---

## 2. Softmax函数
### 数学定义
\[
\text{Softmax}(y_i) = \frac{e^{y_i}}{\sum_{j} e^{y_j}}
\]
- **特性**：
  - 输出值在(0,1)之间，且总和为1。
  - 放大输入logits的差异（如输入[20, 2.7, 0.05] → 输出≈[0.88, 0.12, ≈0]）。

### 二分类的特殊情况
- 当类别数为2时，Softmax等价于**Sigmoid函数**：
  \[
  \text{Sigmoid}(y) = \frac{1}{1 + e^{-y}}
  \]
- 此时，仅需输出一个概率值，另一类别概率为 \(1 - p\)。

---

## 3. 分类问题的损失函数
### 交叉熵损失（Cross-Entropy）
\[
L = -\frac{1}{N} \sum_{n} \sum_{i} \hat{y}_i^{(n)} \ln y_i'^{(n)}
\]
- **标签形式**：类别需表示为**one-hot向量**，如 \(\hat{y} = [1, 0, 0]\)。
- **优势**：
  - 优化目标等价于**最大化似然估计**。
  - 梯度更陡峭，避免均方误差（MSE）在平坦区域的优化停滞问题。

### 均方误差（MSE）的局限性
\[
L_{\text{MSE}} = \frac{1}{N} \sum_{n} \sum_{i} (\hat{y}_i^{(n)} - y_i'^{(n)})^2
\]
- 在分类问题中，MSE易导致**梯度消失**（当预测接近0或1时梯度趋近于0）。

---

## 4. 交叉熵与极大似然的关系
- 假设样本独立，交叉熵最小化等价于最大化数据的**对数似然**：
  \[
  \log P(\text{Data}|\theta) = \sum_{n} \sum_{i} \hat{y}_i^{(n)} \ln y_i'^{(n)}
  \]
- 因此，最小化交叉熵本质是寻找最可能的模型参数 \(\theta\)。

---

## 5. 关键结论
1. **类别编码**：分类标签应表示为one-hot向量，而非直接数值（如1,2,3）。
2. **损失选择**：优先使用交叉熵而非MSE，因其优化更高效。
3. **函数关系**：二分类时，Sigmoid是Softmax的特例，两者数学等价。
4. **优化视角**：交叉熵的梯度方向更明确，适合处理概率分布的差异。

> **参考资料**  
> 李宏毅《机器学习》课程视频：[链接1](https://youtu.be/fZAZUYEelMg)、[链接2](https://youtu.be/hSXFuypLukA)
